{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc40a3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Block 1: Setup and Imports ---\n",
    "\n",
    "# Import libraries we need\n",
    "import numpy as np                # for math & random stuff\n",
    "import matplotlib.pyplot as plt   # for plotting graphs\n",
    "\n",
    "# Make sure we get the same random results every time\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3313796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Block 2: Hidden States and Observations ---\n",
    "\n",
    "# Hidden states (the \"real\" weather we canâ€™t see directly)\n",
    "states = [\"Sunny\", \"Cloudy\", \"Rainy\"]\n",
    "n_states = len(states)\n",
    "\n",
    "# Observations we actually see (like how many umbrellas)\n",
    "observations = [0, 1, 2]  # 0=no umbrellas, 1=some, 2=many\n",
    "n_obs = len(observations)\n",
    "\n",
    "# True probabilities (we use them to generate fake data)\n",
    "true_pi = np.array([0.8, 0.15, 0.05])  # starting chance of each weather\n",
    "true_A = np.array([\n",
    "    [0.7, 0.2, 0.1],  # Sunny -> Sunny, Cloudy, Rainy\n",
    "    [0.3, 0.4, 0.3],  # Cloudy -> Sunny, Cloudy, Rainy\n",
    "    [0.2, 0.3, 0.5]   # Rainy -> Sunny, Cloudy, Rainy\n",
    "])\n",
    "true_B = np.array([\n",
    "    [0.8, 0.15, 0.05],  # Sunny -> few umbrellas\n",
    "    [0.3, 0.5, 0.2],    # Cloudy -> medium umbrellas\n",
    "    [0.1, 0.3, 0.6]     # Rainy -> many umbrellas\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab149c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Block 3: Simulate Data from HMM ---\n",
    "\n",
    "def simulate_hmm(pi, A, B, T=300):\n",
    "    \"\"\"Simulate hidden states and observations for T days\"\"\"\n",
    "    hidden = []\n",
    "    obs = []\n",
    "\n",
    "    # Pick first state randomly using pi\n",
    "    s = np.random.choice(len(pi), p=pi)\n",
    "    hidden.append(s)\n",
    "    o = np.random.choice(len(B[s]), p=B[s])\n",
    "    obs.append(o)\n",
    "\n",
    "    # Repeat for each day\n",
    "    for t in range(1, T):\n",
    "        s = np.random.choice(len(pi), p=A[s])   # next hidden state\n",
    "        hidden.append(s)\n",
    "        o = np.random.choice(len(B[s]), p=B[s]) # observation\n",
    "        obs.append(o)\n",
    "\n",
    "    return np.array(hidden), np.array(obs)\n",
    "\n",
    "# Generate 300 days of fake weather & umbrellas\n",
    "hidden_true, obs = simulate_hmm(true_pi, true_A, true_B, 300)\n",
    "\n",
    "print(\"First 10 true states:\", hidden_true[:10])\n",
    "print(\"First 10 observations:\", obs[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27775c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Block 4: Forward Algorithm (how likely the data is) ---\n",
    "\n",
    "def forward(pi, A, B, obs):\n",
    "    \"\"\"Compute forward probabilities\"\"\"\n",
    "    T = len(obs)\n",
    "    N = len(pi)\n",
    "    alpha = np.zeros((T, N))\n",
    "\n",
    "    # Step 1: initialize\n",
    "    alpha[0] = pi * B[:, obs[0]]\n",
    "\n",
    "    # Step 2: go forward\n",
    "    for t in range(1, T):\n",
    "        for j in range(N):\n",
    "            alpha[t, j] = np.sum(alpha[t-1] * A[:, j]) * B[j, obs[t]]\n",
    "    return alpha\n",
    "\n",
    "# Test it\n",
    "alpha = forward(true_pi, true_A, true_B, obs)\n",
    "print(\"Forward done! Example probs:\", alpha[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4499b95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Block 5: Backward Algorithm (future probabilities) ---\n",
    "\n",
    "def backward(A, B, obs):\n",
    "    \"\"\"Compute backward probabilities\"\"\"\n",
    "    T = len(obs)\n",
    "    N = A.shape[0]\n",
    "    beta = np.zeros((T, N))\n",
    "\n",
    "    # Step 1: start from the end\n",
    "    beta[-1] = 1\n",
    "\n",
    "    # Step 2: go backward\n",
    "    for t in range(T - 2, -1, -1):\n",
    "        for i in range(N):\n",
    "            beta[t, i] = np.sum(A[i, :] * B[:, obs[t + 1]] * beta[t + 1])\n",
    "    return beta\n",
    "\n",
    "# Test it\n",
    "beta = backward(true_A, true_B, obs)\n",
    "print(\"Backward done! Example probs:\", beta[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592b6ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Block 6: Baum-Welch Training (Easier Version) ---\n",
    "\n",
    "def baum_welch(obs, n_states, n_obs, n_iter=10):\n",
    "    \"\"\"Train HMM to learn A, B, pi\"\"\"\n",
    "    T = len(obs)\n",
    "    \n",
    "    # Start with random guesses\n",
    "    pi = np.ones(n_states) / n_states\n",
    "    A = np.ones((n_states, n_states)) / n_states\n",
    "    B = np.ones((n_states, n_obs)) / n_obs\n",
    "\n",
    "    for iteration in range(n_iter):\n",
    "        alpha = forward(pi, A, B, obs)\n",
    "        beta = backward(A, B, obs)\n",
    "        \n",
    "        # gamma = prob of being in state i at time t\n",
    "        gamma = alpha * beta\n",
    "        gamma = gamma / np.sum(gamma, axis=1, keepdims=True)\n",
    "\n",
    "        # xi = prob of being in i then j\n",
    "        xi = np.zeros((T - 1, n_states, n_states))\n",
    "        for t in range(T - 1):\n",
    "            denom = np.sum(alpha[t] @ A * B[:, obs[t + 1]] @ beta[t + 1])\n",
    "            for i in range(n_states):\n",
    "                xi[t, i, :] = alpha[t, i] * A[i, :] * B[:, obs[t + 1]] * beta[t + 1]\n",
    "            xi[t] /= np.sum(xi[t])\n",
    "\n",
    "        # Update parameters\n",
    "        pi = gamma[0]\n",
    "        A = np.sum(xi, axis=0) / np.sum(gamma[:-1], axis=0, keepdims=True).T\n",
    "        for k in range(n_obs):\n",
    "            mask = obs == k\n",
    "            B[:, k] = np.sum(gamma[mask], axis=0)\n",
    "        B = B / np.sum(gamma, axis=0, keepdims=True).T\n",
    "\n",
    "    return pi, A, B\n",
    "\n",
    "# Train the model\n",
    "pi_hat, A_hat, B_hat = baum_welch(obs, n_states, n_obs, n_iter=20)\n",
    "print(\"Training done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aaca7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Block 7: Viterbi Algorithm ---\n",
    "\n",
    "def viterbi(pi, A, B, obs):\n",
    "    \"\"\"Find the most likely hidden states\"\"\"\n",
    "    T = len(obs)\n",
    "    N = len(pi)\n",
    "    delta = np.zeros((T, N))\n",
    "    psi = np.zeros((T, N), dtype=int)\n",
    "\n",
    "    # Init\n",
    "    delta[0] = pi * B[:, obs[0]]\n",
    "\n",
    "    # Forward\n",
    "    for t in range(1, T):\n",
    "        for j in range(N):\n",
    "            probs = delta[t - 1] * A[:, j]\n",
    "            psi[t, j] = np.argmax(probs)\n",
    "            delta[t, j] = np.max(probs) * B[j, obs[t]]\n",
    "\n",
    "    # Backtrack\n",
    "    path = np.zeros(T, dtype=int)\n",
    "    path[-1] = np.argmax(delta[-1])\n",
    "    for t in range(T - 2, -1, -1):\n",
    "        path[t] = psi[t + 1, path[t + 1]]\n",
    "\n",
    "    return path\n",
    "\n",
    "# Decode\n",
    "decoded = viterbi(pi_hat, A_hat, B_hat, obs)\n",
    "print(\"Decoded first 10 states:\", decoded[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ee699d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Block 8: Plot Results ---\n",
    "\n",
    "Tplot = 100\n",
    "\n",
    "plt.figure(figsize=(14, 4))\n",
    "plt.plot(hidden_true[:Tplot], label=\"True states\")\n",
    "plt.plot(decoded[:Tplot], '--', label=\"Predicted (Viterbi)\")\n",
    "plt.legend()\n",
    "plt.title(\"True vs Predicted Hidden States (first 100 days)\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(14, 2))\n",
    "plt.scatter(range(Tplot), obs[:Tplot], s=10)\n",
    "plt.title(\"Observed Umbrella Counts\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7db0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Block 9: Forecasting Next Day\n",
    "# ----------------------------\n",
    "\n",
    "# We'll use the last day's probabilities (how likely each weather state was)\n",
    "# as our \"current state\" distribution\n",
    "last_probs = gamma[-1]  # gamma[-1] = probability of each state on last day\n",
    "\n",
    "# Step 1: Predict next day's hidden state probabilities\n",
    "# Multiply the current state probs by the transition matrix\n",
    "next_state_probs = last_probs @ A_hat\n",
    "\n",
    "# Step 2: Predict the next day's umbrella probabilities\n",
    "# Multiply the predicted state probs by the emission matrix\n",
    "next_umbrella_probs = next_state_probs @ B_hat\n",
    "\n",
    "# Step 3: Print results\n",
    "print(\"Chance of 0 umbrellas tomorrow:\", round(next_umbrella_probs[0], 3))\n",
    "print(\"Chance of 1 umbrella tomorrow:\", round(next_umbrella_probs[1], 3))\n",
    "print(\"Chance of 2 umbrellas tomorrow:\", round(next_umbrella_probs[2], 3))\n",
    "\n",
    "# Step 4: Show a simple bar plot\n",
    "plt.bar([0,1,2], next_umbrella_probs)\n",
    "plt.title(\"Predicted Umbrella Counts for Tomorrow\")\n",
    "plt.xlabel(\"Umbrella Count\")\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72981090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Block 10: Model Selection (BIC)\n",
    "# ----------------------------\n",
    "\n",
    "# We'll test models with K = 2, 3, 4 hidden states\n",
    "# and compare them using the BIC (Bayesian Information Criterion)\n",
    "# Lower BIC = better model fit\n",
    "\n",
    "def compute_bic(log_likelihood, n_params, n_samples):\n",
    "    \"\"\"Small helper to compute BIC\"\"\"\n",
    "    return -2 * log_likelihood + n_params * np.log(n_samples)\n",
    "\n",
    "results = []\n",
    "\n",
    "for K in [2, 3, 4]:\n",
    "    print(f\"\\nTraining model with {K} hidden states...\")\n",
    "    \n",
    "    # Random starting parameters\n",
    "    pi0 = np.ones(K) / K\n",
    "    A0 = np.random.rand(K, K)\n",
    "    A0 /= A0.sum(axis=1, keepdims=True)  # normalize rows\n",
    "    B0 = np.random.rand(K, n_obs)\n",
    "    B0 /= B0.sum(axis=1, keepdims=True)\n",
    "    \n",
    "    # Train using Baum-Welch\n",
    "    pi_k, A_k, B_k, loglik_k, _ = baum_welch(obs, K, n_obs,\n",
    "                                             init_pi=pi0,\n",
    "                                             init_A=A0,\n",
    "                                             init_B=B0,\n",
    "                                             n_iter=100,\n",
    "                                             tol=1e-4)\n",
    "    \n",
    "    # Number of free parameters\n",
    "    n_params = (K - 1) + K * (K - 1) + K * (n_obs - 1)\n",
    "    \n",
    "    # Compute BIC score\n",
    "    bic_val = compute_bic(loglik_k, n_params, len(obs))\n",
    "    results.append((K, loglik_k, bic_val))\n",
    "    \n",
    "    print(f\"Log-likelihood: {loglik_k:.2f}\")\n",
    "    print(f\"BIC: {bic_val:.2f}\")\n",
    "\n",
    "# Show results\n",
    "print(\"\\nFinal Comparison (K, LogLik, BIC):\")\n",
    "for K, loglik, bic in results:\n",
    "    print(f\"{K} states -> LogLik={loglik:.2f}, BIC={bic:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
